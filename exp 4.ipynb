{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2076f64",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e88d866",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c4ce1d",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b12576",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0529c37e",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca6fb2b",
   "metadata": {
    "height": 113
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc4a017",
   "metadata": {
    "height": 164
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "len(initial_tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7949882",
   "metadata": {
    "height": 165
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5bbfb85",
   "metadata": {
    "height": 131
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity scores. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity scores. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
     ]
    }
   ],
   "source": [
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09ae15e6",
   "metadata": {
    "height": 62
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This framework enables the language model to tailor its behavior to diverse task requirements during the inference phase. Self-RAG significantly outperforms state-of-the-art language models and retrieval-augmented models on various tasks, showing improvements in factuality, citation accuracy, and overall generation quality.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models with limited computation cost. It combines shifted sparse attention with LoRA for context extension, allowing models to be fine-tuned to longer context lengths while retaining their original architectures. LongLoRA has been shown to achieve strong empirical results on various tasks and is compatible with existing techniques like Flash-Attention2. It aims to save trainable parameters and memory costs compared to full fine-tuning, and an improved version called LoRA+ has been developed to address performance gaps between LongLoRA and full fine-tuning, especially for long context adaptation scenarios. Additionally, LongLoRA has been used in experiments for tasks such as long-sequence language modeling and topic retrieval, demonstrating promising results and comparable performance to state-of-the-art models.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This framework enables the language model to tailor its behavior to diverse task requirements during the inference phase. Self-RAG significantly outperforms state-of-the-art language models and retrieval-augmented models on various tasks, showing improvements in factuality, citation accuracy, and overall generation quality.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models with limited computation cost. It combines shifted sparse attention with LoRA for context extension, allowing models to be fine-tuned to longer context lengths while retaining their original architectures. LongLoRA has been shown to achieve strong empirical results on various tasks and is compatible with existing techniques like Flash-Attention2. It aims to save trainable parameters and memory costs compared to full fine-tuning, and an improved version called LoRA+ has been developed to address performance gaps between LongLoRA and full fine-tuning, especially for long context adaptation scenarios. Additionally, LongLoRA has been used in experiments for tasks such as long-sequence language modeling and topic retrieval, demonstrating promising results and comparable performance to state-of-the-art models.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This framework enables the language model to tailor its behavior to diverse task requirements during the inference phase. Self-RAG significantly outperforms state-of-the-art language models and retrieval-augmented models on various tasks, showing improvements in factuality, citation accuracy, and overall generation quality.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models with limited computation cost. It combines shifted sparse attention with LoRA for context extension, allowing models to be fine-tuned to longer context lengths while retaining their original architectures. LongLoRA has been shown to achieve strong empirical results on various tasks and is compatible with existing techniques like Flash-Attention2. It aims to save trainable parameters and memory costs compared to full fine-tuning, and an improved version called LoRA+ has been developed to address performance gaps between LongLoRA and full fine-tuning, especially for long context adaptation scenarios. Additionally, LongLoRA has been used in experiments for tasks such as long-sequence language modeling and topic retrieval, demonstrating promising results and comparable performance to state-of-the-art models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683a142",
   "metadata": {
    "height": 29
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
